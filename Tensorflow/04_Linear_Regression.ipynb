{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nDdR3N2R2PJ"
   },
   "source": [
    "<a href=\"https://practicalai.me\"><img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/rounded_logo.png\" width=\"100\" align=\"left\" hspace=\"20px\" vspace=\"20px\"></a>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/basic_ml/04_Linear_Regression/linear_regression.png\" width=\"150\" vspace=\"20px\" align=\"right\">\n",
    "\n",
    "<div align=\"left\">\n",
    "<h1>Linear Regression</h1>\n",
    "\n",
    "In this lesson we will learn about linear regression. We will understand the basic math behind it, implement it in Python. and then look at ways of interpreting the linear model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Izx6GIn_SETR"
   },
   "source": [
    "<table align=\"center\">\n",
    "  <td>\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/rounded_logo.png\" width=\"25\"><a target=\"_blank\" href=\"https://practicalai.me\"> View on practicalAI</a>\n",
    "  </td>\n",
    "  <td>\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/colab_logo.png\" width=\"25\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/04_Linear_Regression.ipynb\"> Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/github_logo.png\" width=\"22\"><a target=\"_blank\" href=\"https://github.com/practicalAI/practicalAI/blob/master/notebooks/basic_ml/04_Linear_Regression.ipynb\"> View code on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoMq0eFRvugb"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaFUStk-24I4"
   },
   "source": [
    "Our goal is to learn a linear model $\\hat{y}$ that models $y$ given $X$. \n",
    "\n",
    "$\\hat{y} = XW + b$\n",
    "* $\\hat{y}$ = predictions | $\\in \\mathbb{R}^{NX1}$ ($N$ is the number of samples)\n",
    "* $X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n",
    "* $W$ = weights | $\\in \\mathbb{R}^{DX1}$ \n",
    "* $b$ = bias | $\\in \\mathbb{R}^{1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAgr7Grv9pb6"
   },
   "source": [
    "* **Objective:**  Use inputs $X$ to predict the output $\\hat{y}$ using a linear model. The model will be a line of best fit that minimizes the distance between the predicted (model's output) and target (ground truth) values. Training data $(X, y)$ is used to train the model and learn the weights $W$ using gradient descent.\n",
    "* **Advantages:**\n",
    "  * Computationally simple.\n",
    "  * Highly interpretable.\n",
    "  * Can account for continuous and categorical features.\n",
    "* **Disadvantages:**\n",
    "  * The model will perform well only when the data is linearly separable (for classification).\n",
    "  * Usually not used for classification and only for regression.\n",
    "* **Miscellaneous:** You can also use linear regression for binary classification tasks where if the predicted continuous value is above a threshold, it belongs to a certain class. But we will cover better techniques for classification in future lessons and will focus on linear regression for continuous regression tasks only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xP7XD24-09Io"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "476yPgTM1BKJ"
   },
   "source": [
    "1. Randomly initialize the model's weights $W$ (we'll cover more effective initalization strategies in future lessons).\n",
    "2. Feed inputs $X$ into the model to receive the predictions $\\hat{y}$.\n",
    "  * $\\hat{y} = XW + b$\n",
    "3. Compare the predictions $\\hat{y}$ with the actual target values $y$ using the objective (cost) function to determine the loss $J$. A common objective function for linear regression is mean squarred error (MSE). This function calculates the difference between the predicted and target values and squares it.\n",
    "\n",
    "  * $J(\\theta) = MSE = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2 $\n",
    "    * ${y}$ = ground truth | $\\in \\mathbb{R}^{NX1}$\n",
    "    * $\\hat{y}$ = predictions | $\\in \\mathbb{R}^{NX1}$\n",
    "4. Calculate the gradient of loss $J(\\theta)$ w.r.t to the model weights.\n",
    "  * $J(\\theta) = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2  = \\frac{1}{N}\\sum_i (y_i - X_iW)^2 $\n",
    "    * $\\frac{\\partial{J}}{\\partial{W}} = -\\frac{2}{N} \\sum_i (y_i - X_iW) X_i = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i) X_i$\n",
    "    * $\\frac{\\partial{J}}{\\partial{b}} = -\\frac{2}{N} \\sum_i (y_i - X_iW)1 = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i)1$\n",
    "5. Update the weights $W$ using a small learning rate $\\alpha$. The simplified intuition is that the gradient tells you the direction for how to increase something so subtracting it will help you go the other way since we want to decrease loss $J(\\theta)$:\n",
    "  * $W = W - \\alpha\\frac{\\partial{J}}{\\partial{W}}$\n",
    "  * $b = b - \\alpha\\frac{\\partial{J}}{\\partial{b}}$\n",
    "6. Repeat steps 2 - 5 to minimize the loss and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqEglvPJTifp"
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ebK53z_VctPM",
    "outputId": "c8ebb86c-8df7-4465-957c-569c2595cbd1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "# Use TensorFlow 2.x\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRXD7LqVJZ43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFsKg-Z6IWqG"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "SEED = 1234\n",
    "SHUFFLE = True\n",
    "NUM_SAMPLES = 50\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "INPUT_DIM = 1\n",
    "HIDDEN_DIM = 1\n",
    "LEARNING_RATE = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K8uaUq8XSsQ8"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvJKjkMeJP4Q"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuPl9qlSJTIY"
   },
   "source": [
    "We're going to create some simple dummy data to apply linear regression on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5T7yGfiEQnx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_a--YWXdwig"
   },
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "McZ7wHsve61A"
   },
   "source": [
    "We're going to create some dummy data to train our linear regression model on. We're going to create roughly linear data (`y = 3.5X + 10`) with some random noise so the points don't all align in a straight line. Our goal is to have the model converge to a similar linear equation (there will be slight variance since we added some noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNWcn3uadzpJ"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "def generate_data(num_samples):\n",
    "    \"\"\"Generate dummy data for linear regression.\"\"\"\n",
    "    X = np.array(range(num_samples))\n",
    "    random_noise = np.random.uniform(-10,20,size=num_samples)\n",
    "    y = 3.5*X + random_noise # add some noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ppGTtKRrdwgF"
   },
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4f9KSJR6flBt"
   },
   "source": [
    "Now let's load the data onto a dataframe and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2mb2SjSQIWvF",
    "outputId": "5a96430d-6776-4356-d685-2ce7fd7ddf26"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.423670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.102416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.954025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>17.727032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>19.817262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X          y\n",
       "0  0.0  -1.423670\n",
       "1  1.0  23.102416\n",
       "2  2.0   9.954025\n",
       "3  3.0  17.727032\n",
       "4  4.0  19.817262"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random (linear) data\n",
    "X, y = generate_data(num_samples=NUM_SAMPLES)\n",
    "data = np.vstack([X, y]).T\n",
    "df = pd.DataFrame(data, columns=['X', 'y'])\n",
    "X = df[['X']].values\n",
    "y = df[['y']].values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "6LwVVOkiLfBN",
    "outputId": "ed7e32c6-c0e2-447c-ad0c-dc4e4d734597"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "plt.title(\"Generated data\")\n",
    "plt.scatter(x=df['X'], y=df['y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qwn29SjK-XCg"
   },
   "source": [
    "# Split data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Z0Nl1Oy3eOJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_bbSMUZ3e3m"
   },
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XAIAT-QoKn1"
   },
   "source": [
    "Since our task is a regression task, we will randomly split our dataset into **three** sets: train, validation and test data splits.\n",
    "\n",
    "* train: used to train our model.\n",
    "* val : used to validate our model's performance during training.\n",
    "* test: used to do an evaluation of our fully trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wIw3j4Z_0ya"
   },
   "source": [
    "<div align=\"left\">\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/lightbulb.gif\" width=\"45px\" align=\"left\" hspace=\"10px\">\n",
    "</div>\n",
    "Splitting the data for classification tasks are a bit different in that we want similar class distributions in each data split. We'll see this in action in the next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKmBKodpgHEE"
   },
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, val_size, test_size, shuffle):\n",
    "    \"\"\"Split data into train/val/test datasets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
    "    \n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, shuffle=shuffle)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2SFTiscx3gNi"
   },
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "WuUQwD72NVAE",
    "outputId": "2eb09cdd-d96a-4242-80d3-20222efa1e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (35, 1), y_train: (35, 1)\n",
      "X_val: (7, 1), y_test: (7, 1)\n",
      "X_test: (8, 1), y_test: (8, 1)\n",
      "X_train[0]: [15.]\n",
      "y_train[0]: [51.00911247]\n"
     ]
    }
   ],
   "source": [
    "# Create data splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y, val_size=VAL_SIZE, test_size=TEST_SIZE, shuffle=SHUFFLE)\n",
    "\n",
    "\n",
    "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print (f\"X_val: {X_val.shape}, y_test: {y_val.shape}\")\n",
    "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print (f\"X_train[0]: {X_train[0]}\")\n",
    "print (f\"y_train[0]: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-Eksj89d_yv"
   },
   "source": [
    "# Standardize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJVs6JF7trja"
   },
   "source": [
    "We need to standardize our data (zero mean and unit variance) in order to optimize quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yD7GFSnvsFfg"
   },
   "source": [
    "$z = \\frac{x_i - \\mu}{\\sigma}$\n",
    "* $z$ = standardized value\n",
    "* $x_i$ = inputs\n",
    "* $\\mu$ = mean\n",
    "* $\\sigma$ = standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlOYPD5GRjRC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiE3oLCkOCEa"
   },
   "outputs": [],
   "source": [
    "# Standardize the data (mean=0, std=1) using training data\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7dKUyWGJ4Av"
   },
   "outputs": [],
   "source": [
    "# Apply scaler on training and test data\n",
    "standardized_X_train = X_scaler.transform(X_train)\n",
    "standardized_y_train = y_scaler.transform(y_train).ravel()\n",
    "standardized_X_val = X_scaler.transform(X_val)\n",
    "standardized_y_val = y_scaler.transform(y_val).ravel()\n",
    "standardized_X_test = X_scaler.transform(X_test)\n",
    "standardized_y_test = y_scaler.transform(y_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "3JC-YFYFJ39Z",
    "outputId": "3cfb0813-841b-4640-bbbf-4715e5afab20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized_X_train: mean: 5.075305255429287e-17, std: 0.9999999999999999\n",
      "standardized_y_train: mean: -1.1419436824715895e-16, std: 0.9999999999999999\n",
      "standardized_X_val: mean: 0.8544098211821912, std: 0.6257842144512111\n",
      "standardized_y_val: mean: 0.9735933655206118, std: 0.6524539116981858\n",
      "standardized_X_test: mean: -0.20997797924415268, std: 0.776904278651562\n",
      "standardized_y_test: mean: -0.251945760795198, std: 0.7798137320223993\n"
     ]
    }
   ],
   "source": [
    "# Check (means should be ~0 and std should be ~1)\n",
    "print (f\"standardized_X_train: mean: {np.mean(standardized_X_train, axis=0)[0]}, std: {np.std(standardized_X_train, axis=0)[0]}\")\n",
    "print (f\"standardized_y_train: mean: {np.mean(standardized_y_train, axis=0)}, std: {np.std(standardized_y_train, axis=0)}\")\n",
    "print (f\"standardized_X_val: mean: {np.mean(standardized_X_val, axis=0)[0]}, std: {np.std(standardized_X_val, axis=0)[0]}\")\n",
    "print (f\"standardized_y_val: mean: {np.mean(standardized_y_val, axis=0)}, std: {np.std(standardized_y_val, axis=0)}\")\n",
    "print (f\"standardized_X_test: mean: {np.mean(standardized_X_test, axis=0)[0]}, std: {np.std(standardized_X_test, axis=0)[0]}\")\n",
    "print (f\"standardized_y_test: mean: {np.mean(standardized_y_test, axis=0)}, std: {np.std(standardized_y_test, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sdruDHf_laWg"
   },
   "source": [
    "# From scratch\n",
    "\n",
    "Before we use TensorFlow 2.0 + Keras we will implent linear regression from scratch using NumPy so we can:\n",
    "1. Absorb the fundamental concepts by implementing from scratch\n",
    "2. Appreciate the level of abstraction TensorFlow provides\n",
    "\n",
    "<div align=\"left\">\n",
    "<img src=\"https://raw.githubusercontent.com/practicalAI/images/master/images/lightbulb.gif\" width=\"45px\" align=\"left\" hspace=\"10px\">\n",
    "</div>\n",
    "\n",
    "It's normal to find the math and code in this section slightly complex. You can still read each of the steps to build intuition for when we implement this using TensorFlow + Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "U4Prl8dDl-T4",
    "outputId": "55ef92c1-4e73-4a63-ff51-9d3965cf5bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (35, 1)\n",
      "y: (35, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.66407964],\n",
       "       [-1.33468496],\n",
       "       [-1.07518349],\n",
       "       [ 1.06699031],\n",
       "       [-0.12049641],\n",
       "       [ 0.87711147],\n",
       "       [ 1.22098308],\n",
       "       [-1.30108031],\n",
       "       [ 0.14476735],\n",
       "       [ 0.5081731 ],\n",
       "       [-0.08167484],\n",
       "       [ 1.51237107],\n",
       "       [ 1.53027096],\n",
       "       [ 1.39509759],\n",
       "       [ 0.06027359],\n",
       "       [-0.19300744],\n",
       "       [-0.87041093],\n",
       "       [ 1.48106887],\n",
       "       [-0.12704813],\n",
       "       [-0.72729706],\n",
       "       [-1.26107446],\n",
       "       [-1.38333096],\n",
       "       [ 0.01740318],\n",
       "       [-0.987311  ],\n",
       "       [-0.7519051 ],\n",
       "       [ 0.35962056],\n",
       "       [ 1.81914562],\n",
       "       [-1.6676142 ],\n",
       "       [ 0.2865345 ],\n",
       "       [-0.24898233],\n",
       "       [-0.80116349],\n",
       "       [ 1.42919148],\n",
       "       [ 0.54439999],\n",
       "       [ 0.54114047],\n",
       "       [-1.19819842]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_y_train = standardized_y_train.reshape(-1, 1)\n",
    "print (f\"X: {standardized_X_train.shape}\")\n",
    "print (f\"y: {standardized_y_train.shape}\")\n",
    "standardized_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8vzjUSW-05x"
   },
   "source": [
    "Our goal is to learn a linear model $\\hat{y}$ that models $y$ given $X$. \n",
    "\n",
    "$\\hat{y} = XW + b$\n",
    "* $\\hat{y}$ = predictions | $\\in \\mathbb{R}^{NX1}$ ($N$ is the number of samples)\n",
    "* $X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n",
    "* $W$ = weights | $\\in \\mathbb{R}^{DX1}$ \n",
    "* $b$ = bias | $\\in \\mathbb{R}^{1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1QzuBFM8by6"
   },
   "source": [
    "1.   Randomly initialize the model's weights $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "KJrD1GiCl-bX",
    "outputId": "43c0157f-47a3-4099-d400-50062b6e45b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: (1, 1)\n",
      "b: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize random weights\n",
    "W = 0.01 * np.random.randn(INPUT_DIM, 1)\n",
    "b = np.zeros((1, 1))\n",
    "print (f\"W: {W.shape}\")\n",
    "print (f\"b: {b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IptdjlS8sBw"
   },
   "source": [
    "2. Feed inputs $X$ into the model to receive the predictions $\\hat{y}$.\n",
    "  * $\\hat{y} = XW + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "_RQsyPW4sSLb",
    "outputId": "449589ec-558f-4925-8621-ce9f652a97c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: (35, 1)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass [NX1] · [1X1] = [NX1]\n",
    "y_hat = np.dot(standardized_X_train, W) + b\n",
    "print (f\"y_hat: {y_hat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5ZTGKol84VO"
   },
   "source": [
    "3. Compare the predictions $\\hat{y}$ with the actual target values $y$ using the objective (cost) function to determine the loss $J$. A common objective function for linear regression is mean squarred error (MSE). This function calculates the difference between the predicted and target values and squares it.\n",
    "\n",
    "  * $J(\\theta) = MSE = \\frac{1}{N} \\sum_{i-1}^{N} (y_i - \\hat{y}_i)^2 $\n",
    "    * ${y}$ = ground truth | $\\in \\mathbb{R}^{NX1}$\n",
    "    * $\\hat{y}$ = predictions | $\\in \\mathbb{R}^{NX1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "Cqs_50xcsSkm",
    "outputId": "eb89f7c8-63c5-49b3-e94e-654519728168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.03\n"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "N = len(standardized_y_train)\n",
    "loss = (1/N) * np.sum((standardized_y_train - y_hat)**2)\n",
    "print (f\"loss: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBQ59qNs90-u"
   },
   "source": [
    "4. Calculate the gradient of loss $J(\\theta)$ w.r.t to the model weights.\n",
    "  * $J(\\theta) = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2  = \\frac{1}{N}\\sum_i (y_i - X_iW)^2 $\n",
    "    * $\\frac{\\partial{J}}{\\partial{W}} = -\\frac{2}{N} \\sum_i (y_i - X_iW) X_i = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i) X_i$\n",
    "    * $\\frac{\\partial{J}}{\\partial{W}} = -\\frac{2}{N} \\sum_i (y_i - X_iW)1 = -\\frac{2}{N} \\sum_i (y_i - \\hat{y}_i)1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NlN9F8bysSiP"
   },
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "dW = -(2/N) * np.sum((standardized_y_train - y_hat) * standardized_X_train)\n",
    "db = -(2/N) * np.sum((standardized_y_train - y_hat) * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vA8K64TF_fRD"
   },
   "source": [
    "5. Update the weights $W$ using a small learning rate $\\alpha$. The simplified intuition is that the gradient tells you the direction for how to increase something so subtracting it will help you go the other way since we want to decrease loss $J(\\theta)$:\n",
    "  * $W = W - \\alpha\\frac{\\partial{J}}{\\partial{W}}$\n",
    "  * $b = b - \\alpha\\frac{\\partial{J}}{\\partial{b}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBa96kSXvDnJ"
   },
   "outputs": [],
   "source": [
    "# Update weights\n",
    "W += -LEARNING_RATE * dW\n",
    "b += -LEARNING_RATE * db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3b0P4Ls6_thq"
   },
   "source": [
    "6. Repeat steps 2 - 5 to minimize the loss and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "xF6_RVuul-ZJ",
    "outputId": "d6dc9f2a-e5ad-465a-d8ed-dd8098bb836a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.015\n",
      "Epoch: 10, loss: 0.043\n",
      "Epoch: 20, loss: 0.032\n",
      "Epoch: 30, loss: 0.032\n",
      "Epoch: 40, loss: 0.032\n",
      "Epoch: 50, loss: 0.032\n",
      "Epoch: 60, loss: 0.032\n",
      "Epoch: 70, loss: 0.032\n",
      "Epoch: 80, loss: 0.032\n",
      "Epoch: 90, loss: 0.032\n"
     ]
    }
   ],
   "source": [
    "# Initialize random weights\n",
    "W = 0.01 * np.random.randn(INPUT_DIM, 1)\n",
    "b = np.zeros((1,1 ))\n",
    "\n",
    "# Training loop\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "\n",
    "    # Forward pass [NX1] · [1X1] = [NX1]\n",
    "    y_hat = np.dot(standardized_X_train, W) + b\n",
    "\n",
    "    # Loss\n",
    "    loss = (1/len(standardized_y_train)) * np.sum((standardized_y_train - y_hat)**2)\n",
    "\n",
    "    # show progress\n",
    "    if epoch_num%10 == 0:\n",
    "        print (f\"Epoch: {epoch_num}, loss: {loss:.3f}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    dW = -(2/N) * np.sum((standardized_y_train - y_hat) * standardized_X_train)\n",
    "    db = -(2/N) * np.sum((standardized_y_train - y_hat) * 1)\n",
    "\n",
    "    # Update weights\n",
    "    W += -LEARNING_RATE * dW\n",
    "    b += -LEARNING_RATE * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcIUx6d-69_g"
   },
   "outputs": [],
   "source": [
    "# Predictions \n",
    "pred_train = W*standardized_X_train + b\n",
    "pred_test = W*standardized_X_test + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "WB_c9ek16-FC",
    "outputId": "2b9b8568-6e6d-48ec-98d9-cb48fedda2dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_MSE: 0.03, test_MSE: 1.19\n"
     ]
    }
   ],
   "source": [
    "# Train and test MSE\n",
    "train_mse = np.mean((standardized_y_train - pred_train) ** 2)\n",
    "test_mse = np.mean((standardized_y_test - pred_test) ** 2)\n",
    "print (f\"train_MSE: {train_mse:.2f}, test_MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "by9uqJbC699J",
    "outputId": "52652814-cfa3-4d39-8d58-a085c2cab0ea",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# Plot train data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plt.scatter(standardized_X_train, standardized_y_train, label='y_train')\n",
    "plt.plot(standardized_X_train, pred_train, color='red', linewidth=1, linestyle='-', label='model')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot test data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plt.scatter(standardized_X_test, standardized_y_test, label='y_test')\n",
    "plt.plot(standardized_X_test, pred_test, color='red', linewidth=1, linestyle='-', label='model')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3MlFkZ8_der"
   },
   "source": [
    "Since we standardized our inputs and outputs, our weights were fit to those standardized values. So we need to unstandardize our weights so we can compare it to our true weight (3.5).\n",
    "\n",
    "Note that both X and y were standardized.\n",
    "\n",
    "$\\hat{y}_{scaled} = b_{scaled} + \\sum_{j=1}^{k}W_{{scaled}_j}x_{{scaled}_j}$\n",
    "* $y_{scaled} = \\frac{\\hat{y} - \\bar{y}}{\\sigma_y}$\n",
    "* $x_{scaled} = \\frac{x_j - \\bar{x}_j}{\\sigma_j}$\n",
    "\n",
    "$\\frac{\\hat{y} - \\bar{y}}{\\sigma_y} = b_{scaled} + \\sum_{j=1}^{k}W_{{scaled}_j}\\frac{x_j - \\bar{x}_j}{\\sigma_j}$\n",
    "\n",
    "$ \\hat{y}_{scaled} = \\frac{\\hat{y}_{unscaled} - \\bar{y}}{\\sigma_y} = {b_{scaled}} + \\sum_{j=1}^{k} {W}_{{scaled}_j} (\\frac{x_j - \\bar{x}_j}{\\sigma_j}) $\n",
    "\n",
    "$\\hat{y}_{unscaled} = b_{scaled}\\sigma_y + \\bar{y} - \\sum_{j=1}^{k} {W}_{{scaled}_j}(\\frac{\\sigma_y}{\\sigma_j})\\bar{x}_j + \\sum_{j=1}^{k}{W}_{{scaled}_j}(\\frac{\\sigma_y}{\\sigma_j})x_j $\n",
    "\n",
    "In the expression above, we can see the expression $\\hat{y}_{unscaled} = W_{unscaled}x + b_{unscaled} $ where\n",
    "\n",
    "* $W_{unscaled} = \\sum_{j=1}^{k}{W}_j(\\frac{\\sigma_y}{\\sigma_j}) $\n",
    "\n",
    "* $b_{unscaled} = b_{scaled}\\sigma_y + \\bar{y} - \\sum_{j=1}^{k} {W}_j(\\frac{\\sigma_y}{\\sigma_j})\\bar{x}_j$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "C-PcjG4G2TRq",
    "outputId": "74ac88ec-1c1a-4747-bd82-7902f9318bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[actual] y = 3.5X + noise\n",
      "[model] y_hat = 3.4779679763175992X + [4.91778952]\n"
     ]
    }
   ],
   "source": [
    "# Unscaled weights\n",
    "W_unscaled = W * (y_scaler.scale_/X_scaler.scale_)\n",
    "b_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)\n",
    "print (\"[actual] y = 3.5X + noise\")\n",
    "print (f\"[model] y_hat = {W_unscaled[0][0]}X + {b_unscaled[0]}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LD5m4v1A9nqF"
   },
   "source": [
    "Now let's implement linear regression with TensorFlow + Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "33NIijOMKqZF"
   },
   "source": [
    "# TensorFlow + Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCuIxAH7KuwG"
   },
   "source": [
    "We will be using [Dense layers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) in our MLP implementation. The layer applies an activation function on the dot product of the layer's inputs and its weights.\n",
    "\n",
    "$ z = \\text{activation}(XW)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QvjSlcsfKqjY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "07hfw6dcK_jo",
    "outputId": "c05d40b6-a1ec-4bfc-b72f-c102fde37ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z (None, 1) = x (None, 1) · W (1, 1) + b (1,)\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=(INPUT_DIM,))\n",
    "fc = Dense(units=HIDDEN_DIM, activation='linear')\n",
    "z = fc(x)\n",
    "W, b = fc.weights\n",
    "print (f\"z {z.shape} = x {x.shape} · W {W.shape} + b {b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4drnbzryeVsD"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGH_pQaDOb49"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import MeanAbsolutePercentageError\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-3zRdoIeaas"
   },
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "class LinearRegression(Model):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.fc1 = Dense(units=hidden_dim, activation='linear')\n",
    "        \n",
    "    def call(self, x_in, training=False):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        y_pred = self.fc1(x_in)\n",
    "        return y_pred\n",
    "    \n",
    "    def sample(self, input_shape):\n",
    "        x_in = Input(shape=input_shape)\n",
    "        return Model(inputs=x_in, outputs=self.call(x_in)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "8y3YIttkeaX2",
    "outputId": "c1b26629-a5b8-47bc-c446-4a85de7e2c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LinearRegression(hidden_dim=HIDDEN_DIM)\n",
    "model.sample(input_shape=(INPUT_DIM,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yj5snqjN0Y4j"
   },
   "source": [
    "When we implemented linear regression from scratch, we used batch gradient descent to update our weights. But there are actually many different [gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/) to choose from and it depends on the situation. However, the [ADAM optimizer](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms/#adam) has become a standard algorithm for most cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eZiOOYeerCV"
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(optimizer=Adam(lr=LEARNING_RATE),\n",
    "              loss=MeanSquaredError(),\n",
    "              metrics=[MeanAbsolutePercentageError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysKlKW_lki9R"
   },
   "source": [
    "<img height=\"45\" src=\"http://bestanimations.com/HomeOffice/Lights/Bulbs/animated-light-bulb-gif-29.gif\" align=\"left\" vspace=\"5px\" hspace=\"10px\">\n",
    "\n",
    "Here are the full list of options for [optimizer](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers), [loss](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses) and [metrics](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VtxXsNq3hrz"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWn-wFPEFu38"
   },
   "source": [
    "When we implemented linear regression from scratch, we used batch gradient descent to update our weights. This means that we calculated the gradients using the entire training dataset. We also could've updated our weights using stochastic gradient descent (SGD) where we pass in one training example at a time. The current standard is mini-batch gradient descent, which strikes a balance between batch and stochastic GD, where we update the weights using a mini-batch of n (`BATCH_SIZE`) samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "u_ZOiIGleq_l",
    "outputId": "81046735-603d-48ce-abc3-0c4b42b88ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35 samples, validate on 7 samples\n",
      "Epoch 1/100\n",
      "35/35 [==============================] - 1s 29ms/sample - loss: 2.2303 - mean_absolute_percentage_error: 189.2383 - val_loss: 1.8631 - val_mean_absolute_percentage_error: 110.1029\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 0s 604us/sample - loss: 1.1683 - mean_absolute_percentage_error: 129.5676 - val_loss: 0.8827 - val_mean_absolute_percentage_error: 85.6471\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 0s 650us/sample - loss: 0.5139 - mean_absolute_percentage_error: 72.2302 - val_loss: 0.3088 - val_mean_absolute_percentage_error: 63.4733\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 0s 675us/sample - loss: 0.1775 - mean_absolute_percentage_error: 58.3600 - val_loss: 0.0744 - val_mean_absolute_percentage_error: 43.0542\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 0s 635us/sample - loss: 0.0590 - mean_absolute_percentage_error: 55.3533 - val_loss: 0.0241 - val_mean_absolute_percentage_error: 29.1076\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 0s 724us/sample - loss: 0.0501 - mean_absolute_percentage_error: 52.8501 - val_loss: 0.0298 - val_mean_absolute_percentage_error: 21.7504\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 0s 718us/sample - loss: 0.0740 - mean_absolute_percentage_error: 54.2517 - val_loss: 0.0345 - val_mean_absolute_percentage_error: 17.8960\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 0s 685us/sample - loss: 0.0897 - mean_absolute_percentage_error: 51.1155 - val_loss: 0.0292 - val_mean_absolute_percentage_error: 17.8972\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 0s 693us/sample - loss: 0.0826 - mean_absolute_percentage_error: 46.4416 - val_loss: 0.0204 - val_mean_absolute_percentage_error: 16.6245\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 0s 745us/sample - loss: 0.0588 - mean_absolute_percentage_error: 41.9437 - val_loss: 0.0161 - val_mean_absolute_percentage_error: 16.1506\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 0s 735us/sample - loss: 0.0367 - mean_absolute_percentage_error: 38.9170 - val_loss: 0.0205 - val_mean_absolute_percentage_error: 22.4666\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 0s 719us/sample - loss: 0.0285 - mean_absolute_percentage_error: 38.1202 - val_loss: 0.0309 - val_mean_absolute_percentage_error: 28.5176\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 0s 727us/sample - loss: 0.0314 - mean_absolute_percentage_error: 38.2243 - val_loss: 0.0406 - val_mean_absolute_percentage_error: 31.7614\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 0s 702us/sample - loss: 0.0355 - mean_absolute_percentage_error: 37.9354 - val_loss: 0.0450 - val_mean_absolute_percentage_error: 32.1937\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 0s 699us/sample - loss: 0.0358 - mean_absolute_percentage_error: 36.5390 - val_loss: 0.0433 - val_mean_absolute_percentage_error: 30.6931\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 0s 631us/sample - loss: 0.0333 - mean_absolute_percentage_error: 35.0792 - val_loss: 0.0375 - val_mean_absolute_percentage_error: 28.4353\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 0s 626us/sample - loss: 0.0305 - mean_absolute_percentage_error: 34.9512 - val_loss: 0.0306 - val_mean_absolute_percentage_error: 26.3657\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 0s 634us/sample - loss: 0.0289 - mean_absolute_percentage_error: 35.9414 - val_loss: 0.0253 - val_mean_absolute_percentage_error: 24.9581\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 0s 659us/sample - loss: 0.0284 - mean_absolute_percentage_error: 37.1964 - val_loss: 0.0226 - val_mean_absolute_percentage_error: 24.2765\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 0s 682us/sample - loss: 0.0283 - mean_absolute_percentage_error: 37.8857 - val_loss: 0.0221 - val_mean_absolute_percentage_error: 24.1799\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 0s 725us/sample - loss: 0.0283 - mean_absolute_percentage_error: 37.7560 - val_loss: 0.0233 - val_mean_absolute_percentage_error: 24.4904\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 0s 754us/sample - loss: 0.0282 - mean_absolute_percentage_error: 37.1553 - val_loss: 0.0253 - val_mean_absolute_percentage_error: 25.0431\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 0s 646us/sample - loss: 0.0284 - mean_absolute_percentage_error: 36.6211 - val_loss: 0.0271 - val_mean_absolute_percentage_error: 25.6636\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 0s 604us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.4372 - val_loss: 0.0282 - val_mean_absolute_percentage_error: 26.1652\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 0s 721us/sample - loss: 0.0288 - mean_absolute_percentage_error: 36.5204 - val_loss: 0.0284 - val_mean_absolute_percentage_error: 26.4022\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 0s 824us/sample - loss: 0.0288 - mean_absolute_percentage_error: 36.6643 - val_loss: 0.0280 - val_mean_absolute_percentage_error: 26.3404\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 0s 869us/sample - loss: 0.0287 - mean_absolute_percentage_error: 36.7185 - val_loss: 0.0274 - val_mean_absolute_percentage_error: 26.0752\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 0s 1ms/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6859 - val_loss: 0.0269 - val_mean_absolute_percentage_error: 25.7720\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 0s 715us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.6579 - val_loss: 0.0265 - val_mean_absolute_percentage_error: 25.5705\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 0s 659us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.6927 - val_loss: 0.0263 - val_mean_absolute_percentage_error: 25.5202\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 0s 679us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7627 - val_loss: 0.0263 - val_mean_absolute_percentage_error: 25.5831\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 0s 677us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.8006 - val_loss: 0.0265 - val_mean_absolute_percentage_error: 25.6859\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 0s 773us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7741 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7713\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 0s 788us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7127 - val_loss: 0.0270 - val_mean_absolute_percentage_error: 25.8174\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 0s 727us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6695 - val_loss: 0.0270 - val_mean_absolute_percentage_error: 25.8273\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 0s 678us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6699 - val_loss: 0.0270 - val_mean_absolute_percentage_error: 25.8115\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 0s 655us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6975 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7807\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 0s 688us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7206 - val_loss: 0.0267 - val_mean_absolute_percentage_error: 25.7473\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 0s 669us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7241 - val_loss: 0.0267 - val_mean_absolute_percentage_error: 25.7243\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 0s 811us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7156 - val_loss: 0.0267 - val_mean_absolute_percentage_error: 25.7198\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 0s 761us/sample - loss: 0.0285 - mean_absolute_percentage_error: 36.7096 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7316\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 0s 639us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7101 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7490\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 0s 689us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7116 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7608\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 0s 710us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7089 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7625\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 0s 648us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7040 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7570\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 0s 667us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7018 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7497\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 0s 676us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7040 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7444\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 0s 647us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7073 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7419\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 0s 648us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7081 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7416\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 0s 724us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7062 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7428\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 0s 748us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7037 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7443\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 0s 832us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7026 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7454\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 0s 761us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7026 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7450\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 0s 683us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7025 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7435\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 0s 678us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7019 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7415\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 0s 708us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7013 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7400\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 0s 679us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7010 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7394\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 0s 641us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7008 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7392\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 0s 701us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.7003 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7390\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 0s 713us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6995 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7387\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 0s 747us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6988 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7381\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 0s 685us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6983 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7374\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 0s 639us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6980 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7366\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 0s 701us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6976 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7358\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 0s 722us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6971 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7351\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 0s 689us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6965 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7346\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 0s 643us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6961 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7341\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 0s 673us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6956 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7336\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 0s 644us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6951 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7330\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 0s 705us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6946 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7323\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 0s 673us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6941 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7317\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 0s 680us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6937 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7311\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 0s 626us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6932 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7305\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 0s 801us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6927 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7300\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 0s 627us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6922 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7294\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 0s 658us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6918 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7288\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 0s 762us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6913 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7283\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 0s 757us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6908 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7277\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 0s 739us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6904 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7271\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 0s 764us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6899 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7266\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 0s 716us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6894 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7260\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 0s 757us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6890 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7255\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 0s 836us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6885 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7249\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 0s 754us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6881 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7244\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 0s 765us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6876 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7238\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 0s 741us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6872 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7233\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 0s 686us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6867 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7227\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 0s 828us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6862 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7222\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 0s 730us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6858 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7217\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 0s 711us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6853 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7211\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 0s 755us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6849 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7206\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 0s 799us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6844 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7201\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 0s 914us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6840 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7196\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 0s 733us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6836 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7190\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 0s 768us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6831 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7185\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 0s 755us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6827 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7180\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 0s 750us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6822 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7175\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 0s 1ms/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6818 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7170\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 0s 719us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6813 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7165\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 0s 779us/sample - loss: 0.0286 - mean_absolute_percentage_error: 36.6809 - val_loss: 0.0268 - val_mean_absolute_percentage_error: 25.7160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9faf1326d8>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(x=standardized_X_train, \n",
    "          y=standardized_y_train,\n",
    "          validation_data=(standardized_X_val, standardized_y_val),\n",
    "          epochs=NUM_EPOCHS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          shuffle=False,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRl3Gvu8ewD4"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhdu9lBie_t_"
   },
   "source": [
    "There are several evaluation techniques to see how well our model performed. A common one for linear regression is mean squarred error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZl9Jz8qetzI"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "pred_train = model.predict(standardized_X_train)\n",
    "pred_test = model.predict(standardized_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UT30FvMUetwe",
    "outputId": "24168a89-3644-4583-9a39-72b791f776be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_MSE: 0.03, test_MSE: 2.21\n"
     ]
    }
   ],
   "source": [
    "# Train and test MSE\n",
    "train_mse = np.mean((standardized_y_train - pred_train) ** 2)\n",
    "test_mse = np.mean((standardized_y_test - pred_test) ** 2)\n",
    "print (f\"train_MSE: {train_mse:.2f}, test_MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TegkJM2-YKEq"
   },
   "source": [
    "Since we only have one feature, it's easy to visually inspect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "I8nuCOjGeySz",
    "outputId": "f3cca3ea-7428-4ce3-e3d6-8f5e0487f54f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAE/CAYAAAAKZVEAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# Plot train data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plt.scatter(standardized_X_train, standardized_y_train, label='y_train')\n",
    "plt.plot(standardized_X_train, pred_train, color='red', linewidth=1, linestyle='-', label='model')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot test data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plt.scatter(standardized_X_test, standardized_y_test, label='y_test')\n",
    "plt.plot(standardized_X_test, pred_test, color='red', linewidth=1, linestyle='-', label='model')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T621buxbe0VQ"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4W84oQzfNn_"
   },
   "source": [
    "After training a model, we can use it to predict on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_ZKW-p2e0fN"
   },
   "outputs": [],
   "source": [
    "# Feed in your own inputs\n",
    "sample_indices = [10, 15, 25]\n",
    "X_infer = np.array(sample_indices, dtype=np.float32)\n",
    "standardized_X_infer = X_scaler.transform(X_infer.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZjQ54zKazbE"
   },
   "source": [
    "Recall that we need to unstandardize our predictions.\n",
    "\n",
    "$ \\hat{y}_{scaled} = \\frac{\\hat{y} - \\mu_{\\hat{y}}}{\\sigma_{\\hat{y}}} $\n",
    "\n",
    "$ \\hat{y} = \\hat{y}_{scaled} * \\sigma_{\\hat{y}} + \\mu_{\\hat{y}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PMAdz712aE34",
    "outputId": "aed82cf8-86fe-4fc0-8971-884b8ca27b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.73 (actual) → 43.49 (predicted)\n",
      "59.34 (actual) → 60.03 (predicted)\n",
      "97.04 (actual) → 93.09 (predicted)\n"
     ]
    }
   ],
   "source": [
    "# Unstandardize predictions\n",
    "pred_infer = model.predict(standardized_X_infer) * np.sqrt(y_scaler.var_) + y_scaler.mean_\n",
    "for i, index in enumerate(sample_indices):\n",
    "    print (f\"{df.iloc[index]['y']:.2f} (actual) → {pred_infer[i][0]:.2f} (predicted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5CB4zRFe37l"
   },
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhXo8CbPBZ-G"
   },
   "source": [
    "Linear regression offers the great advantage of being highly interpretable. Each feature has a coefficient which signifies it's importance/impact on the output variable y. We can interpret our coefficient as follows: by increasing X by 1 unit, we increase y by $W$ (~3.65) units. \n",
    "\n",
    "**Note**: Since we standardized our inputs and outputs for gradient descent, we need to apply an operation to our coefficients and intercept to interpret them. See proof in the `From scratch` section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lJOOjjLze6_U",
    "outputId": "2a735554-e3cd-42bc-db32-c83b0cc8ea8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[actual] y = 3.5X + noise\n",
      "[model] y_hat = 3.3X + 10.4\n"
     ]
    }
   ],
   "source": [
    "# Unstandardize coefficients (proof is in the `From Scratch` section above)\n",
    "W = model.layers[0].get_weights()[0][0][0]\n",
    "b = model.layers[0].get_weights()[1][0]\n",
    "W_unscaled = W * (y_scaler.scale_/X_scaler.scale_)\n",
    "b_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)\n",
    "print (\"[actual] y = 3.5X + noise\")\n",
    "print (f\"[model] y_hat = {W_unscaled[0]:.1f}X + {b_unscaled[0]:.1f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rToCXKqeJcvj"
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4GFv8xRJmOZ"
   },
   "source": [
    "Regularization helps decrease overfitting. Below is L2 regularization (ridge regression). There are many forms of regularization but they all work to reduce overfitting in our models. With L2 regularization, we are penalizing the weights with large magnitudes by decaying them. Having certain weights with high magnitudes will lead to preferential bias with the inputs and we want the model to work with all the inputs and not just a select few. There are also other types of regularization like L1 (lasso regression) which is useful for creating sparse models where some feature cofficients are zeroed out, or elastic which combines L1 and L2 penalties. \n",
    "\n",
    "**Note**: Regularization is not just for linear regression. You can use it to regularize any model's weights including the ones we will look at in future lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_OcpRxF-Oj7"
   },
   "source": [
    "$ J(\\theta) = = \\frac{1}{2}\\sum_{i}(X_iW - y_i)^2 + \\frac{\\lambda}{2}W^TW$\n",
    "\n",
    "$ \\frac{\\partial{J}}{\\partial{W}}  = X (\\hat{y} - y) + \\lambda W $\n",
    "\n",
    "$W = W- \\alpha\\frac{\\partial{J}}{\\partial{W}}$\n",
    "* $\\lambda$ is the regularzation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHaazL9f8QZX"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTIUZLbGZP4e"
   },
   "outputs": [],
   "source": [
    "L2_LAMBDA = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORwkUqcuZhbX"
   },
   "outputs": [],
   "source": [
    "# Linear model with L2 regularization\n",
    "class LinearRegressionL2Regularization(Model):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(LinearRegressionL2Regularization, self).__init__()\n",
    "        self.fc1 = Dense(units=hidden_dim, activation='linear',\n",
    "                        kernel_regularizer=l2(l=L2_LAMBDA))\n",
    "        \n",
    "    def call(self, x_in, training=False):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        y_pred = self.fc1(x_in)\n",
    "        return y_pred\n",
    "    \n",
    "    def sample(self, input_shape):\n",
    "        x_in = Input(shape=input_shape)\n",
    "        return Model(inputs=x_in, outputs=self.call(x_in)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "IWCvYxBxZhd5",
    "outputId": "3670f885-252f-49fd-afc1-8c64a9328003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LinearRegressionL2Regularization(hidden_dim=HIDDEN_DIM)\n",
    "model.sample(input_shape=(INPUT_DIM,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ-XpSYAoNBX"
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(optimizer=Adam(lr=LEARNING_RATE),\n",
    "              loss=MeanSquaredError(),\n",
    "              metrics=[MeanAbsolutePercentageError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Xlt9IuaCoM-X",
    "outputId": "f771fbe4-a860-42cc-8550-a6f7d1e42a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35 samples, validate on 7 samples\n",
      "Epoch 1/100\n",
      "35/35 [==============================] - 0s 12ms/sample - loss: 0.3207 - mean_absolute_percentage_error: 63.4316 - val_loss: 0.1373 - val_mean_absolute_percentage_error: 47.1876\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 0s 681us/sample - loss: 0.0765 - mean_absolute_percentage_error: 56.2515 - val_loss: 0.0292 - val_mean_absolute_percentage_error: 25.8445\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 0s 763us/sample - loss: 0.0624 - mean_absolute_percentage_error: 46.6229 - val_loss: 0.0400 - val_mean_absolute_percentage_error: 17.3323\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 0s 764us/sample - loss: 0.0910 - mean_absolute_percentage_error: 46.5634 - val_loss: 0.0335 - val_mean_absolute_percentage_error: 16.3579\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 0s 768us/sample - loss: 0.0680 - mean_absolute_percentage_error: 43.0348 - val_loss: 0.0262 - val_mean_absolute_percentage_error: 20.1073\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 0s 745us/sample - loss: 0.0402 - mean_absolute_percentage_error: 41.6203 - val_loss: 0.0416 - val_mean_absolute_percentage_error: 29.3647\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 0s 732us/sample - loss: 0.0441 - mean_absolute_percentage_error: 36.3268 - val_loss: 0.0667 - val_mean_absolute_percentage_error: 33.7147\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 0s 786us/sample - loss: 0.0507 - mean_absolute_percentage_error: 34.8555 - val_loss: 0.0582 - val_mean_absolute_percentage_error: 31.9228\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 0s 859us/sample - loss: 0.0435 - mean_absolute_percentage_error: 36.0092 - val_loss: 0.0379 - val_mean_absolute_percentage_error: 28.8918\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 0s 747us/sample - loss: 0.0383 - mean_absolute_percentage_error: 39.8574 - val_loss: 0.0293 - val_mean_absolute_percentage_error: 23.9395\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 0s 764us/sample - loss: 0.0436 - mean_absolute_percentage_error: 44.1764 - val_loss: 0.0290 - val_mean_absolute_percentage_error: 20.0000\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 0s 823us/sample - loss: 0.0420 - mean_absolute_percentage_error: 36.9044 - val_loss: 0.0303 - val_mean_absolute_percentage_error: 23.3255\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 0s 831us/sample - loss: 0.0380 - mean_absolute_percentage_error: 39.9483 - val_loss: 0.0337 - val_mean_absolute_percentage_error: 26.4391\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 0s 918us/sample - loss: 0.0383 - mean_absolute_percentage_error: 38.9423 - val_loss: 0.0384 - val_mean_absolute_percentage_error: 27.6047\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 0s 826us/sample - loss: 0.0380 - mean_absolute_percentage_error: 35.2859 - val_loss: 0.0423 - val_mean_absolute_percentage_error: 26.1917\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 0s 724us/sample - loss: 0.0394 - mean_absolute_percentage_error: 33.7857 - val_loss: 0.0404 - val_mean_absolute_percentage_error: 24.4440\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 0s 671us/sample - loss: 0.0391 - mean_absolute_percentage_error: 33.6447 - val_loss: 0.0368 - val_mean_absolute_percentage_error: 24.8876\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 0s 752us/sample - loss: 0.0390 - mean_absolute_percentage_error: 39.0913 - val_loss: 0.0306 - val_mean_absolute_percentage_error: 25.0254\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 0s 808us/sample - loss: 0.0397 - mean_absolute_percentage_error: 40.4396 - val_loss: 0.0316 - val_mean_absolute_percentage_error: 24.8026\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 0s 781us/sample - loss: 0.0383 - mean_absolute_percentage_error: 39.0221 - val_loss: 0.0341 - val_mean_absolute_percentage_error: 26.1505\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 0s 668us/sample - loss: 0.0381 - mean_absolute_percentage_error: 37.9055 - val_loss: 0.0353 - val_mean_absolute_percentage_error: 25.8629\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 0s 716us/sample - loss: 0.0385 - mean_absolute_percentage_error: 37.9434 - val_loss: 0.0365 - val_mean_absolute_percentage_error: 24.8363\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 0s 632us/sample - loss: 0.0384 - mean_absolute_percentage_error: 37.1647 - val_loss: 0.0360 - val_mean_absolute_percentage_error: 25.6325\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 0s 684us/sample - loss: 0.0382 - mean_absolute_percentage_error: 36.5625 - val_loss: 0.0354 - val_mean_absolute_percentage_error: 26.1762\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 0s 677us/sample - loss: 0.0384 - mean_absolute_percentage_error: 37.5807 - val_loss: 0.0317 - val_mean_absolute_percentage_error: 24.9922\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 0s 634us/sample - loss: 0.0392 - mean_absolute_percentage_error: 39.7816 - val_loss: 0.0314 - val_mean_absolute_percentage_error: 23.6788\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 0s 677us/sample - loss: 0.0382 - mean_absolute_percentage_error: 37.7479 - val_loss: 0.0336 - val_mean_absolute_percentage_error: 25.8214\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 0s 635us/sample - loss: 0.0396 - mean_absolute_percentage_error: 39.1087 - val_loss: 0.0391 - val_mean_absolute_percentage_error: 28.0559\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 0s 681us/sample - loss: 0.0400 - mean_absolute_percentage_error: 32.6750 - val_loss: 0.0426 - val_mean_absolute_percentage_error: 24.6492\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 0s 696us/sample - loss: 0.0404 - mean_absolute_percentage_error: 35.4442 - val_loss: 0.0345 - val_mean_absolute_percentage_error: 25.2430\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 0s 799us/sample - loss: 0.0399 - mean_absolute_percentage_error: 42.5052 - val_loss: 0.0350 - val_mean_absolute_percentage_error: 26.3707\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 0s 674us/sample - loss: 0.0394 - mean_absolute_percentage_error: 32.0588 - val_loss: 0.0482 - val_mean_absolute_percentage_error: 26.7856\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 0s 671us/sample - loss: 0.0416 - mean_absolute_percentage_error: 36.8677 - val_loss: 0.0370 - val_mean_absolute_percentage_error: 26.2617\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 0s 655us/sample - loss: 0.0389 - mean_absolute_percentage_error: 36.6695 - val_loss: 0.0351 - val_mean_absolute_percentage_error: 25.8119\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 0s 644us/sample - loss: 0.0381 - mean_absolute_percentage_error: 37.4172 - val_loss: 0.0343 - val_mean_absolute_percentage_error: 26.7241\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 0s 736us/sample - loss: 0.0388 - mean_absolute_percentage_error: 40.5537 - val_loss: 0.0324 - val_mean_absolute_percentage_error: 26.5931\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 0s 678us/sample - loss: 0.0384 - mean_absolute_percentage_error: 39.3858 - val_loss: 0.0346 - val_mean_absolute_percentage_error: 24.6551\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 0s 687us/sample - loss: 0.0382 - mean_absolute_percentage_error: 37.2376 - val_loss: 0.0393 - val_mean_absolute_percentage_error: 24.0049\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 0s 678us/sample - loss: 0.0412 - mean_absolute_percentage_error: 32.9674 - val_loss: 0.0370 - val_mean_absolute_percentage_error: 24.2000\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 0s 664us/sample - loss: 0.0391 - mean_absolute_percentage_error: 41.1487 - val_loss: 0.0304 - val_mean_absolute_percentage_error: 25.3624\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 0s 743us/sample - loss: 0.0398 - mean_absolute_percentage_error: 42.0001 - val_loss: 0.0343 - val_mean_absolute_percentage_error: 25.7569\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 0s 778us/sample - loss: 0.0381 - mean_absolute_percentage_error: 38.6668 - val_loss: 0.0376 - val_mean_absolute_percentage_error: 27.1323\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 0s 698us/sample - loss: 0.0382 - mean_absolute_percentage_error: 37.3946 - val_loss: 0.0403 - val_mean_absolute_percentage_error: 26.5623\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 0s 651us/sample - loss: 0.0393 - mean_absolute_percentage_error: 33.6592 - val_loss: 0.0365 - val_mean_absolute_percentage_error: 24.2606\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 0s 645us/sample - loss: 0.0396 - mean_absolute_percentage_error: 35.6431 - val_loss: 0.0296 - val_mean_absolute_percentage_error: 24.2568\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 0s 696us/sample - loss: 0.0390 - mean_absolute_percentage_error: 40.5911 - val_loss: 0.0349 - val_mean_absolute_percentage_error: 26.9537\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 0s 900us/sample - loss: 0.0397 - mean_absolute_percentage_error: 38.6967 - val_loss: 0.0381 - val_mean_absolute_percentage_error: 27.8834\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 0s 695us/sample - loss: 0.0403 - mean_absolute_percentage_error: 40.7488 - val_loss: 0.0383 - val_mean_absolute_percentage_error: 23.7432\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 0s 717us/sample - loss: 0.0390 - mean_absolute_percentage_error: 34.6170 - val_loss: 0.0366 - val_mean_absolute_percentage_error: 25.7099\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 0s 870us/sample - loss: 0.0411 - mean_absolute_percentage_error: 37.6178 - val_loss: 0.0330 - val_mean_absolute_percentage_error: 27.1938\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 0s 756us/sample - loss: 0.0387 - mean_absolute_percentage_error: 40.9319 - val_loss: 0.0375 - val_mean_absolute_percentage_error: 25.5870\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 0s 750us/sample - loss: 0.0418 - mean_absolute_percentage_error: 39.4853 - val_loss: 0.0421 - val_mean_absolute_percentage_error: 24.4247\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 0s 788us/sample - loss: 0.0387 - mean_absolute_percentage_error: 41.1079 - val_loss: 0.0347 - val_mean_absolute_percentage_error: 27.6516\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 0s 816us/sample - loss: 0.0417 - mean_absolute_percentage_error: 45.7743 - val_loss: 0.0307 - val_mean_absolute_percentage_error: 25.7710\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 0s 719us/sample - loss: 0.0389 - mean_absolute_percentage_error: 40.6163 - val_loss: 0.0331 - val_mean_absolute_percentage_error: 22.6059\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 0s 796us/sample - loss: 0.0419 - mean_absolute_percentage_error: 33.3361 - val_loss: 0.0394 - val_mean_absolute_percentage_error: 24.5434\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 0s 675us/sample - loss: 0.0379 - mean_absolute_percentage_error: 32.3632 - val_loss: 0.0346 - val_mean_absolute_percentage_error: 28.2802\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 0s 975us/sample - loss: 0.0415 - mean_absolute_percentage_error: 42.8242 - val_loss: 0.0326 - val_mean_absolute_percentage_error: 26.0543\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 0s 669us/sample - loss: 0.0396 - mean_absolute_percentage_error: 35.1262 - val_loss: 0.0464 - val_mean_absolute_percentage_error: 23.1342\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 0s 757us/sample - loss: 0.0410 - mean_absolute_percentage_error: 33.7063 - val_loss: 0.0423 - val_mean_absolute_percentage_error: 27.7429\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 0s 805us/sample - loss: 0.0396 - mean_absolute_percentage_error: 40.5701 - val_loss: 0.0361 - val_mean_absolute_percentage_error: 28.5229\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 0s 1ms/sample - loss: 0.0424 - mean_absolute_percentage_error: 41.5852 - val_loss: 0.0309 - val_mean_absolute_percentage_error: 22.5578\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 0s 674us/sample - loss: 0.0386 - mean_absolute_percentage_error: 37.2645 - val_loss: 0.0360 - val_mean_absolute_percentage_error: 24.8106\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 0s 716us/sample - loss: 0.0409 - mean_absolute_percentage_error: 36.2904 - val_loss: 0.0455 - val_mean_absolute_percentage_error: 29.0923\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 0s 731us/sample - loss: 0.0432 - mean_absolute_percentage_error: 43.8714 - val_loss: 0.0334 - val_mean_absolute_percentage_error: 27.6204\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 0s 649us/sample - loss: 0.0422 - mean_absolute_percentage_error: 47.8032 - val_loss: 0.0334 - val_mean_absolute_percentage_error: 20.9902\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 0s 906us/sample - loss: 0.0415 - mean_absolute_percentage_error: 36.2853 - val_loss: 0.0326 - val_mean_absolute_percentage_error: 24.4588\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 0s 678us/sample - loss: 0.0371 - mean_absolute_percentage_error: 37.7962 - val_loss: 0.0389 - val_mean_absolute_percentage_error: 28.5098\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 0s 675us/sample - loss: 0.0395 - mean_absolute_percentage_error: 38.2019 - val_loss: 0.0427 - val_mean_absolute_percentage_error: 28.8511\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 0s 673us/sample - loss: 0.0399 - mean_absolute_percentage_error: 36.1976 - val_loss: 0.0415 - val_mean_absolute_percentage_error: 26.5284\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 0s 663us/sample - loss: 0.0378 - mean_absolute_percentage_error: 34.8055 - val_loss: 0.0316 - val_mean_absolute_percentage_error: 23.6727\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 0s 641us/sample - loss: 0.0395 - mean_absolute_percentage_error: 38.6054 - val_loss: 0.0296 - val_mean_absolute_percentage_error: 23.1205\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 0s 658us/sample - loss: 0.0400 - mean_absolute_percentage_error: 42.1685 - val_loss: 0.0361 - val_mean_absolute_percentage_error: 27.0867\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 0s 674us/sample - loss: 0.0380 - mean_absolute_percentage_error: 37.8014 - val_loss: 0.0447 - val_mean_absolute_percentage_error: 25.8171\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 0s 691us/sample - loss: 0.0409 - mean_absolute_percentage_error: 33.3442 - val_loss: 0.0449 - val_mean_absolute_percentage_error: 26.9463\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 0s 956us/sample - loss: 0.0389 - mean_absolute_percentage_error: 34.4400 - val_loss: 0.0324 - val_mean_absolute_percentage_error: 24.4327\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 0s 678us/sample - loss: 0.0386 - mean_absolute_percentage_error: 36.9492 - val_loss: 0.0308 - val_mean_absolute_percentage_error: 25.4985\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 0s 670us/sample - loss: 0.0396 - mean_absolute_percentage_error: 39.2027 - val_loss: 0.0301 - val_mean_absolute_percentage_error: 23.4169\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 0s 705us/sample - loss: 0.0389 - mean_absolute_percentage_error: 38.8427 - val_loss: 0.0322 - val_mean_absolute_percentage_error: 24.2110\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 0s 743us/sample - loss: 0.0378 - mean_absolute_percentage_error: 37.4581 - val_loss: 0.0378 - val_mean_absolute_percentage_error: 26.6263\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 0s 707us/sample - loss: 0.0388 - mean_absolute_percentage_error: 35.4535 - val_loss: 0.0455 - val_mean_absolute_percentage_error: 28.0387\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 0s 650us/sample - loss: 0.0397 - mean_absolute_percentage_error: 36.1303 - val_loss: 0.0357 - val_mean_absolute_percentage_error: 25.1957\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 0s 811us/sample - loss: 0.0385 - mean_absolute_percentage_error: 38.7578 - val_loss: 0.0314 - val_mean_absolute_percentage_error: 24.3773\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 0s 728us/sample - loss: 0.0384 - mean_absolute_percentage_error: 37.5197 - val_loss: 0.0352 - val_mean_absolute_percentage_error: 25.3775\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 0s 679us/sample - loss: 0.0394 - mean_absolute_percentage_error: 37.3282 - val_loss: 0.0381 - val_mean_absolute_percentage_error: 27.2179\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 0s 667us/sample - loss: 0.0381 - mean_absolute_percentage_error: 36.6569 - val_loss: 0.0329 - val_mean_absolute_percentage_error: 25.2801\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 0s 718us/sample - loss: 0.0391 - mean_absolute_percentage_error: 40.6502 - val_loss: 0.0279 - val_mean_absolute_percentage_error: 22.8830\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 0s 837us/sample - loss: 0.0399 - mean_absolute_percentage_error: 40.3445 - val_loss: 0.0337 - val_mean_absolute_percentage_error: 24.7839\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 0s 677us/sample - loss: 0.0382 - mean_absolute_percentage_error: 37.4946 - val_loss: 0.0481 - val_mean_absolute_percentage_error: 28.2170\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 0s 720us/sample - loss: 0.0400 - mean_absolute_percentage_error: 36.0338 - val_loss: 0.0419 - val_mean_absolute_percentage_error: 27.3621\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 0s 617us/sample - loss: 0.0391 - mean_absolute_percentage_error: 35.8857 - val_loss: 0.0324 - val_mean_absolute_percentage_error: 23.9461\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 0s 701us/sample - loss: 0.0397 - mean_absolute_percentage_error: 40.2459 - val_loss: 0.0304 - val_mean_absolute_percentage_error: 23.9141\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 0s 626us/sample - loss: 0.0398 - mean_absolute_percentage_error: 38.5679 - val_loss: 0.0395 - val_mean_absolute_percentage_error: 26.4940\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 0s 748us/sample - loss: 0.0391 - mean_absolute_percentage_error: 34.6138 - val_loss: 0.0363 - val_mean_absolute_percentage_error: 24.9717\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 0s 692us/sample - loss: 0.0382 - mean_absolute_percentage_error: 35.4741 - val_loss: 0.0342 - val_mean_absolute_percentage_error: 26.1449\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 0s 662us/sample - loss: 0.0393 - mean_absolute_percentage_error: 39.5697 - val_loss: 0.0320 - val_mean_absolute_percentage_error: 24.9750\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 0s 644us/sample - loss: 0.0395 - mean_absolute_percentage_error: 38.5554 - val_loss: 0.0372 - val_mean_absolute_percentage_error: 26.9449\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 0s 675us/sample - loss: 0.0430 - mean_absolute_percentage_error: 37.0881 - val_loss: 0.0346 - val_mean_absolute_percentage_error: 22.9008\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 0s 715us/sample - loss: 0.0422 - mean_absolute_percentage_error: 34.1944 - val_loss: 0.0338 - val_mean_absolute_percentage_error: 27.3413\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 0s 711us/sample - loss: 0.0389 - mean_absolute_percentage_error: 39.3782 - val_loss: 0.0333 - val_mean_absolute_percentage_error: 24.8787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9fad2f7a58>"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(x=standardized_X_train, \n",
    "          y=standardized_y_train,\n",
    "          validation_data=(standardized_X_val, standardized_y_val),\n",
    "          epochs=NUM_EPOCHS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          shuffle=SHUFFLE,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNB5k_MIoPGv"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "pred_train = model.predict(standardized_X_train)\n",
    "pred_test = model.predict(standardized_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f8jVRQhuoPEI",
    "outputId": "591d72a8-8f52-4b3d-cfa8-c6bba55f04ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_MSE: 0.03, test_MSE: 2.24\n"
     ]
    }
   ],
   "source": [
    "# Train and test MSE\n",
    "train_mse = np.mean((standardized_y_train - pred_train) ** 2)\n",
    "test_mse = np.mean((standardized_y_test - pred_test) ** 2)\n",
    "print (f\"train_MSE: {train_mse:.2f}, test_MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-4X3GZEdoR-8",
    "outputId": "eea237a4-a2b3-4d40-f24f-6c64082fda37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[actual] y = 3.5X + noise\n",
      "[model] y_hat = 3.4X + 9.5\n"
     ]
    }
   ],
   "source": [
    "# Unstandardize coefficients (proof is in the `From Scratch` section above)\n",
    "W = model.layers[0].get_weights()[0][0][0]\n",
    "b = model.layers[0].get_weights()[1][0]\n",
    "W_unscaled = W * (y_scaler.scale_/X_scaler.scale_)\n",
    "b_unscaled = b * y_scaler.scale_ + y_scaler.mean_ - np.sum(W_unscaled*X_scaler.mean_)\n",
    "print (\"[actual] y = 3.5X + noise\")\n",
    "print (f\"[model] y_hat = {W_unscaled[0]:.1f}X + {b_unscaled[0]:.1f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdNX2W5eh2ma"
   },
   "source": [
    "Regularization didn't help much with this specific example because our data is generated from a perfect linear equation but for large realistic data, regularization can help our model generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V74lNFE5v5pQ"
   },
   "source": [
    "# Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2r6Xhyg7v5vX"
   },
   "source": [
    "In our example, the feature was a continuous variable but what if we also have features that are categorical? One option is to treat the categorical variables as one-hot encoded variables. This is very easy to do with Pandas and once you create the dummy variables, you can use the same steps as above to train your linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "unhcIOfMxQEQ",
    "outputId": "458bfea8-d946-4cd6-cf1c-5943a952008c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_letter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  favorite_letter\n",
       "0               a\n",
       "1               b\n",
       "2               c\n",
       "3               a"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data with categorical features\n",
    "cat_data = pd.DataFrame(['a', 'b', 'c', 'a'], columns=['favorite_letter'])\n",
    "cat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "m4eQmJdrxQGr",
    "outputId": "b6bf3b36-c210-4587-b73a-617c9da6f6ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_letter_a</th>\n",
       "      <th>favorite_letter_b</th>\n",
       "      <th>favorite_letter_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   favorite_letter_a  favorite_letter_b  favorite_letter_c\n",
       "0                  1                  0                  0\n",
       "1                  0                  1                  0\n",
       "2                  0                  0                  1\n",
       "3                  1                  0                  0"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_cat_data = pd.get_dummies(cat_data)\n",
    "dummy_cat_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5R8x-KyiBWJ"
   },
   "source": [
    "Now you can concat this with your continuous features and train the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwSZgi_pCb3Q"
   },
   "source": [
    "---\n",
    "<div align=\"center\">\n",
    "\n",
    "Subscribe to our <a href=\"https://practicalai.me/#newsletter\">newsletter</a> and follow us on social media to get the latest updates!\n",
    "\n",
    "<a class=\"ai-header-badge\" target=\"_blank\" href=\"https://github.com/practicalAI/practicalAI\">\n",
    "              <img src=\"https://img.shields.io/github/stars/practicalAI/practicalAI.svg?style=social&label=Star\"></a>&nbsp;\n",
    "            <a class=\"ai-header-badge\" target=\"_blank\" href=\"https://www.linkedin.com/company/practicalai-me\">\n",
    "              <img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n",
    "            <a class=\"ai-header-badge\" target=\"_blank\" href=\"https://twitter.com/practicalAIme\">\n",
    "              <img src=\"https://img.shields.io/twitter/follow/practicalAIme.svg?label=Follow&style=social\">\n",
    "            </a>\n",
    "              </div>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sdruDHf_laWg"
   ],
   "name": "04_Linear_Regression",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
